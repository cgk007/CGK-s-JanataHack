{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/test_vc2kHdQ.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/train_SOaYf6m/train.csv\")\n",
    "\n",
    "train_path = \"/kaggle/input/train_SOaYf6m/images/\"\n",
    "test_path = \"/kaggle/input/train_SOaYf6m/images/\"\n",
    "\n",
    "print(train.emergency_or_not.value_counts())\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "indexes = train.index\n",
    "training_data = []\n",
    "\n",
    "size = 224\n",
    "\n",
    "path_file = train_path\n",
    "\n",
    "def create_training_data():\n",
    "    for i in indexes:\n",
    "        path = os.path.join(path_file, train.loc[i, 'image_names'])\n",
    "#         print(\"Image No: \",i, )\n",
    "        class_idx = train.loc[i, 'emergency_or_not']\n",
    "\n",
    "        img_array = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        new_array = cv2.resize(img_array, (size, size))\n",
    "        training_data.append([new_array, class_idx])\n",
    "\n",
    "create_training_data() \n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for a, b in training_data:\n",
    "    x.append(a)\n",
    "    y.append(b)\n",
    "\n",
    "# del training_data\n",
    "\n",
    "X = np.array(x).reshape(-1, size, size, 3)\n",
    "\n",
    "# del x\n",
    "\n",
    "# plt.imshow(X[0])\n",
    "# plt.show()\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# pickle_out = open(\"X.pickle\",\"wb\")\n",
    "# pickle.dump(X, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y.pickle\",\"wb\")\n",
    "# pickle.dump(y, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# del X, y\n",
    "\n",
    "# pickle_out = open(\"y_train.pickle\",\"wb\")\n",
    "# pickle.dump(y_train, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# pickle_out = open(\"y_test.pickle\",\"wb\")\n",
    "# pickle.dump(y_test, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# __________________________________________________________________________________________________________\n",
    "indexes = test.index\n",
    "test_data = []\n",
    "\n",
    "size = 224\n",
    "\n",
    "path_file = test_path\n",
    "\n",
    "def create_test_data():\n",
    "    for i in indexes:\n",
    "        path = os.path.join(path_file, test.loc[i, 'image_names'])\n",
    "        \n",
    "        # class_idx = train.loc[i, 'Class']\n",
    "#         print(\"Image No: \",i,\" !!!\")\n",
    "        img_array = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        new_array = cv2.resize(img_array, (size, size))\n",
    "        test_data.append([new_array, 0])\n",
    "\n",
    "create_test_data()\n",
    "\n",
    "x_pred = []\n",
    "y_pred = []\n",
    "\n",
    "for a, b in test_data:\n",
    "    x_pred.append(a)\n",
    "    y_pred.append(b)\n",
    "\n",
    "X_pred = np.array(x_pred).reshape(-1, size, size, 3)\n",
    "print(X_pred.shape)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# pickle_out = open(\"X_pred.pickle\",\"wb\")\n",
    "# pickle.dump(X_pred, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 201\n",
    "\n",
    "img_array = X[index]\n",
    "plt.imshow(img_array)\n",
    "plt.show()\n",
    "\n",
    "print(y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "\n",
    "NCLASSES = 2\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# ResNet152\n",
    "# base_model = tf.keras.applications.VGG19(input_shape=(HEIGHT, WIDTH, NUM_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "# ResNet50\n",
    "# base_model = tf.keras.applications.ResNet50(input_shape=(HEIGHT, WIDTH, NUM_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "# ResNet101\n",
    "base_model = tf.keras.applications.ResNet101(input_shape=(HEIGHT, WIDTH, NUM_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "# EfficientNetB7\n",
    "# base_model = tf.keras.applications.DenseNet201(input_shape=(HEIGHT, WIDTH, NUM_CHANNELS), include_top=False, weights='imagenet')\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "tf.compat.v2.random.set_seed(1)\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(1024, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_3 = models.Model(inputs=base_model.input, outputs=x)\n",
    "# print(model_3.summary())\n",
    "\n",
    "# X = np.array(pickle.load(open(\"X.pickle\", \"rb\")))\n",
    "# y = np.array(pickle.load(open(\"y.pickle\", \"rb\")))\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32),steps_per_epoch=len(X_train) / 32, epochs=34, \n",
    "#                       validation_data = [np.array(X_test), np.array(y_test)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, random_state = 21, test_size=0.1)\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# num_classes = 2\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# train_gen = datagen.fit(X_train, augment=True, seed=1)\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# resnet50\n",
    "model_3.fit(np.array(X_train), np.array(y_train), epochs = 50, validation_data = [np.array(X_test), np.array(y_test)])\n",
    "# 35\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32), epochs=37,, validation_data = [np.array(X_test), np.array(y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_3.predict([X_pred])\n",
    "pred_class = np.argmax(pred, axis = 1)\n",
    "pred_class.shape\n",
    "temp = pd.DataFrame(pred_class)\n",
    "temp.to_csv(\"resnet101_train_layers_1024_48.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "tf.compat.v2.random.set_seed(1)\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_3 = models.Model(inputs=base_model.input, outputs=x)\n",
    "# print(model_3.summary())\n",
    "\n",
    "# X = np.array(pickle.load(open(\"X.pickle\", \"rb\")))\n",
    "# y = np.array(pickle.load(open(\"y.pickle\", \"rb\")))\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32),steps_per_epoch=len(X_train) / 32, epochs=34, \n",
    "#                       validation_data = [np.array(X_test), np.array(y_test)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, random_state = 21, test_size=0.1)\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# num_classes = 2\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# train_gen = datagen.fit(X_train, augment=True, seed=1)\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# resnet50\n",
    "model_3.fit(np.array(X_train), np.array(y_train), epochs = 29, validation_data = [np.array(X_test), np.array(y_test)])\n",
    "# 13, 15, 29, 30, 41, 49\n",
    "\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32), epochs=37,, validation_data = [np.array(X_test), np.array(y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "\n",
    "tf.compat.v2.random.set_seed(123)\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_3 = models.Model(inputs=base_model.input, outputs=x)\n",
    "# print(model_3.summary())\n",
    "\n",
    "# X = np.array(pickle.load(open(\"X.pickle\", \"rb\")))\n",
    "# y = np.array(pickle.load(open(\"y.pickle\", \"rb\")))\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32),steps_per_epoch=len(X_train) / 32, epochs=34, \n",
    "#                       validation_data = [np.array(X_test), np.array(y_test)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, random_state = 21, test_size=0.1)\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# num_classes = 2\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=5,\n",
    "#     width_shift_range=0.05,\n",
    "#     height_shift_range=0.05,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# datagen.fit(X_train, augment=True, seed=1)\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_3.fit(np.array(X_train), np.array(y_train), epochs = 31, validation_data = [np.array(X_test), np.array(y_test)])\n",
    "\n",
    "# model_3.fit_generator(datagen.flow(X_train, y_train, batch_size=32), epochs=56, validation_data = [np.array(X_test), np.array(y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_3.predict([X_pred])\n",
    "pred_class = np.argmax(pred, axis = 1)\n",
    "pred_class.shape\n",
    "temp = pd.DataFrame(pred_class)\n",
    "temp.to_csv(\"resnet101_train_layers_1024_epochs_31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v2.random.set_seed(1)\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# # x = layers.Dense(128, activation='relu')(x)\n",
    "# # x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_f = models.Model(inputs=base_model.input, outputs=x)\n",
    "model_f.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_f.fit(np.array(X), np.array(y), epochs=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1b: XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_res = X.reshape(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "x = StandardScaler().fit_transform(X_res)\n",
    "print(x.shape)\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(x)\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(x_pca, y, random_state = 21)\n",
    "\n",
    "model = XGBClassifier(tree_method = 'gpu_hist')\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "num_classes = 2\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_3.predict([X_pred])\n",
    "pred_class = np.argmax(pred, axis = 1)\n",
    "pred_class.shape\n",
    "temp = pd.DataFrame(pred_class)\n",
    "temp.to_csv(\"vgg16_layers_5_epochs_34.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Augmentation Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
    "    return image, label\n",
    "\n",
    "def augment(image,label):\n",
    "    image,label = convert(image, label)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding\n",
    "    image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_batches = (\n",
    "    train_dataset\n",
    "    # Only train on a subset, so you can quickly see the effect.\n",
    "    .take(NUM_EXAMPLES)\n",
    "    .cache()\n",
    "    .shuffle(num_train_examples//4)\n",
    "    # The augmentation is added here.\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_aug = make_model()\n",
    "\n",
    "aug_history = model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
